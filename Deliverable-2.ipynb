{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "del2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPS+RY1tejlFruOmyRWMTPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/little-silence/MAIS-202/blob/master/Deliverable-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZr0P5htDrGP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Deliverable 2**\n",
        "\n",
        "\n",
        "\n",
        "1. Problem statement:\n",
        "\n",
        "  The goal of this project is to associate a doodle with the english word equivalent of what it represents.\n",
        "\n",
        "2. Data Preprocessing:\n",
        "\n",
        "  I will be using the categories.txt file provided by the Google Creative Lab GitHub to get the objects for which we have the data.\n",
        "https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt\n",
        "\n",
        "  In the same repository, they have also provided the datasets in four different format. I will be working with the Numpy Bitmaps provided.\n",
        "\n",
        "  https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap\n",
        "\n",
        "  This preprocessed data provides a centered drawing(final result), rescaled to a 28x28 image represented in by a matrix of grayscale values of the same size. In otherwords, it allows us to skip the data cleaning step.\n",
        "\n",
        "3. Machine learning model:\n",
        "\n",
        "  In the first deliverable, I have proposed a decision tree paired with stroke recognition. As my TPM pointed out, a neural network is more suitable for this scenario.\n",
        "\n",
        "  The model I have chosen to implement is the Convolutional Neural Network. It capture what I have been wanting to achieve: recognizing the different features in a doodle to guess the object drawn with the help of filters. I have been following code from the following address to write the model I will be using\n",
        "\n",
        "  https://github.com/zaidalyafeai/zaidalyafeai.github.io/blob/master/sketcher/Sketcher.ipynb\n",
        "\n",
        "  Since the Google dataset contains so muh data, I have decided to only run my test on only 20 object categ ories, and 6000 datapoints from each to allow for faster training. For the final version, it will be best to train on a larger dataset. I opted for a 60:40 split for train+valid and test due to the number of datapoints available, and 80:20 split within the train and validation set. The number of convolutional layers, as well as the output dimensions, and the filter sizes, and hyperparameters have been taken from the tutorial. To optimize, the loss function proposed is the Adam Optimizer. It is a stochastic function, which performs well with non static data and noise. This seems like an appropriate choice with the amount of noise in the dataset. \n",
        "\n",
        "  The model at this point seems a little overfit. Although The concept of this model has been understood, I still need to understand the specifics, such as the number of layers, size of filters, optimization function, and many other hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeJFlGr8ojOV",
        "colab_type": "code",
        "outputId": "8a105f28-d5a9-4e77-b203-4d14b22e6b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# import the google drawing categories\n",
        "!wget 'https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-16 17:52:11--  https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2791 (2.7K) [text/plain]\n",
            "Saving to: ‘categories.txt’\n",
            "\n",
            "\rcategories.txt        0%[                    ]       0  --.-KB/s               \rcategories.txt      100%[===================>]   2.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-02-16 17:52:11 (80.0 MB/s) - ‘categories.txt’ saved [2791/2791]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSdy1kSphgEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"categories.txt\",\"r\")\n",
        "# And for reading use\n",
        "categories = f.readlines()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYqjRb1Chg4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ba1c7ae3-05ae-4859-fa86-c30432d286b0"
      },
      "source": [
        "category = [c.replace('\\n','').replace(' ','_') for c in categories]\n",
        "# There is too much data, so for now, I will only be analysing the 20 first categories\n",
        "short_length = 20\n",
        "short = category[:short_length]\n",
        "print(short)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['aircraft_carrier', 'airplane', 'alarm_clock', 'ambulance', 'angel', 'animal_migration', 'ant', 'anvil', 'apple', 'arm', 'asparagus', 'axe', 'backpack', 'banana', 'bandage', 'barn', 'baseball', 'baseball_bat', 'basket', 'basketball']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCilOQeXhhNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4votpJ9qcgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "#Get the data from Google Quickdraw\n",
        "def download():\n",
        "  \n",
        "  base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "  for element in short:\n",
        "    end_url = element.replace('_', '%20')\n",
        "    path = base+end_url+'.npy'\n",
        "    print(path)\n",
        "    urllib.request.urlretrieve(path, 'data/'+element+'.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuYtumHlqgAB",
        "colab_type": "code",
        "outputId": "4623f617-2da8-475e-af28-f567698b230c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "download()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/aircraft%20carrier.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/alarm%20clock.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ambulance.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/angel.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/animal%20migration.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ant.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/anvil.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/arm.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/asparagus.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/axe.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/backpack.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bandage.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/barn.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball%20bat.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/basket.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/basketball.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmh5Sva5xfK9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "027c81fb-5e2e-4c38-b502-113298255566"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras \n",
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIXkCWmzxoKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_data(root, vfold_ratio=0.2, max_items_per_class= 50000 ):\n",
        "    # get names of all .npy files in str wrapper\n",
        "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
        "    #find the features of this dataset\n",
        "    temp = np.load(all_files[0], allow_pickle=True)\n",
        "    dim = len(temp[0])\n",
        "    \n",
        "\n",
        "    #initialize variables\n",
        "    x = np.empty([0,dim])\n",
        "    y = np.empty([0])\n",
        "    class_names = []\n",
        "\n",
        "    #load a subset of the data to memory \n",
        "    for idx, file in enumerate(all_files):\n",
        "        data = np.load(file)\n",
        "        try:\n",
        "          # take the specified number of data\n",
        "          data = data[: max_items_per_class]\n",
        "        except:\n",
        "          # when number specified is larger\n",
        "          pass\n",
        "        labels = np.full(data.shape[0], idx)\n",
        "\n",
        "        # append to variable\n",
        "        x = np.concatenate((x, data), axis=0)\n",
        "        y = np.append(y, labels)\n",
        "\n",
        "        class_name = (file.replace(\".npy\",'')).replace(\"data/\",'')\n",
        "        class_names.append(class_name)\n",
        "\n",
        "    data = None\n",
        "    labels = None\n",
        "\n",
        "    #shuffle & separate into training and testing \n",
        "    permutation = np.random.permutation(y.shape[0])\n",
        "    x = x[permutation]\n",
        "    y = y[permutation]\n",
        "\n",
        "    vfold_size = int(len(x)*vfold_ratio)\n",
        "\n",
        "    x_test = x[0:vfold_size]\n",
        "    y_test = y[0:vfold_size]\n",
        "\n",
        "    x_train = x[vfold_size:x.shape[0]]\n",
        "    y_train = y[vfold_size:y.shape[0]]\n",
        "    return x_train, y_train, x_test, y_test, class_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMsLaX2HzWW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "x_train, y_train, x_test, y_test, class_names = load_data('data',0.4,6000)\n",
        "num_classes = len(class_names)\n",
        "image_size = int(math.sqrt(len(x_train[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwyKKiYPBsU7",
        "colab_type": "code",
        "outputId": "9bd25aba-eb63-49d3-d17f-9d595478bd5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "%matplotlib inline  \n",
        "idx = randint(0, len(x_train))\n",
        "plt.imshow(x_train[idx].reshape(28,28)) \n",
        "print(class_names[int(y_train[idx].item())])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "asparagus\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIOxAEIQdCIKwA0EQdiAIwg4E8X9cVtOs9MSbdgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8YYX_CmB8xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape and normalize\n",
        "x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')\n",
        "\n",
        "# pixel value is within range [0,255] we want the value to be less than 1\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "# Convert class vectors to class matrices\n",
        "# so we can do categorical cross entropy(used when one answer out of all categories is right)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLj65uBc_DCh",
        "colab_type": "text"
      },
      "source": [
        "I still dont understand why the pooling layer parameter, I will have to look into it soon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbu9JBq1CMe_",
        "colab_type": "code",
        "outputId": "4d36ebe1-4d12-4863-cfe2-764f05ffdb0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "#Construct Model\n",
        "model = keras.Sequential()\n",
        "# first conv layer outputs 16 features with 3 by 3 filter\n",
        "model.add(layers.Convolution2D(16, (3, 3),\n",
        "                        padding='same',\n",
        "                        input_shape=x_train.shape[1:], activation='relu'))\n",
        "# pooling layer summarizes what the conv layer finds\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Convolution2D(32, (3, 3), padding='same', activation= 'relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Convolution2D(64, (3, 3), padding='same', activation= 'relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size =(2,2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "# conclude with softmax to find node with highest value\n",
        "model.add(layers.Dense(short_length, activation='softmax')) \n",
        "# Train model\n",
        "# adam is a stochastic function, good for changing result and good with noisy data\n",
        "adam = tf.train.AdamOptimizer()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['top_k_categorical_accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               73856     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                2580      \n",
            "=================================================================\n",
            "Total params: 99,732\n",
            "Trainable params: 99,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj9yudtWCWmu",
        "colab_type": "code",
        "outputId": "020ac5eb-a206-4867-d1f5-6350854e1a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "model.fit(x = x_train, y = y_train, validation_split=0.2, batch_size = 256, verbose=2, epochs=5)\n",
        "\n",
        "#evaluate on test data\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy: {:0.2f}%'.format(score[1] * 100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 57600 samples, validate on 14400 samples\n",
            "Epoch 1/5\n",
            "57600/57600 - 43s - loss: 1.5266 - top_k_categorical_accuracy: 0.8428 - val_loss: 1.0423 - val_top_k_categorical_accuracy: 0.9227\n",
            "Epoch 2/5\n",
            "57600/57600 - 43s - loss: 0.8965 - top_k_categorical_accuracy: 0.9380 - val_loss: 0.8121 - val_top_k_categorical_accuracy: 0.9463\n",
            "Epoch 3/5\n",
            "57600/57600 - 43s - loss: 0.7589 - top_k_categorical_accuracy: 0.9491 - val_loss: 0.7366 - val_top_k_categorical_accuracy: 0.9510\n",
            "Epoch 4/5\n",
            "57600/57600 - 43s - loss: 0.6842 - top_k_categorical_accuracy: 0.9538 - val_loss: 0.6970 - val_top_k_categorical_accuracy: 0.9519\n",
            "Epoch 5/5\n",
            "57600/57600 - 43s - loss: 0.6332 - top_k_categorical_accuracy: 0.9576 - val_loss: 0.6575 - val_top_k_categorical_accuracy: 0.9549\n",
            "48000/48000 [==============================] - 17s 345us/sample - loss: 0.6697 - top_k_categorical_accuracy: 0.9529\n",
            "Test accuracy: 95.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CLShvSPP3FK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73b33141-2e69-4e45-bf84-1728457f39a8"
      },
      "source": [
        "# confusion matrix creation. \n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "y_test_predict = model.predict(x_test)\n",
        "y_pred= (y_test_predict>0.5) \n",
        "\n",
        "\n",
        "results = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(y_pred, axis=1)) \n",
        "print ('Confusion Matrix :')\n",
        "print(results) \n",
        "print ('Accuracy Score :',accuracy_score(y_test, y_pred) )\n",
        "print ('Report : ')\n",
        "print (classification_report(y_test, y_pred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[2286    4    0    1    4    7    5   11    6    2    1    0    0   11\n",
            "     1    4   12    2    0    1]\n",
            " [ 432 1864    2   17   12   22    2    8    3    2    0    4   14   24\n",
            "     2    3   30    6    6    3]\n",
            " [ 476    2 1758    2    1   63    1   44    0    2    3    1    3   32\n",
            "     0    1   38    0    5    9]\n",
            " [ 286    2    1 1960    5   12    2   16    2    1    1   21   54    7\n",
            "     1    1   23    1   19    5]\n",
            " [ 332    3    0    2 1893   34   29   18   27    8    2    4    1    5\n",
            "     2    5    9   12    8    1]\n",
            " [ 794   21  193    5   24 1121    5   23    2   10   19   15    8   31\n",
            "     1    6   60    3    5   14]\n",
            " [ 353    2    1    3   42   10 1894   11    6    4    0    2    2   25\n",
            "     2   10   12   22    6    1]\n",
            " [ 253    2    8    6    3    3    1 2025    3    2    0    2    1   17\n",
            "     0    6   11    2    2    2]\n",
            " [ 462    3    0    1   37    0    3   23 1609    2    0    6    4    7\n",
            "   194   16   11   45    2    0]\n",
            " [  72    1    0    0    2    5    1    4    0 2321    1    4    1    2\n",
            "     0   30   14    0    2    0]\n",
            " [ 247    3    6    1    9   37    8   14    1    4 2012    2    3    8\n",
            "     1    4   28    0   10    1]\n",
            " [ 121    2    3   25    2    4    0    7    0    3    0 2155   15    2\n",
            "     2    0    5    0    6    7]\n",
            " [ 259    2    0   55    2    5    0    4    2    1    1   25 1957    1\n",
            "     2    1   28    1   21    9]\n",
            " [ 276    2    6    2    3    2    7   23    1    1    3    3    1 1880\n",
            "     0    3  139    0    9    4]\n",
            " [ 397    1    1    3    7    1    5   12  182    4    0   10   25    3\n",
            "  1592    8   20   66   12    6]\n",
            " [ 328   10    0    0   13    9    9   35   13   78    1    4    0   24\n",
            "     4 1852   19    4    3    4]\n",
            " [ 335    0    2    7   11   19    4   14    3    2    2    0    4   67\n",
            "     2    3 1913    3    7   13]\n",
            " [ 544    2    0    5   59    2   40   24   46   17    1    7    1    2\n",
            "   100   36    4 1578    6    1]\n",
            " [ 251    0    2   53    3    3    6    8    1    1    3    2   17    8\n",
            "     0    2   42    4 1947    4]\n",
            " [ 700    3   14   58    1   28    2    6    4    0    1  290   67   14\n",
            "     1    2   90    3   38 1099]]\n",
            "Accuracy Score : 0.7605416666666667\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.88      0.91      2358\n",
            "           1       0.97      0.76      0.85      2456\n",
            "           2       0.88      0.72      0.79      2441\n",
            "           3       0.89      0.81      0.85      2420\n",
            "           4       0.89      0.79      0.84      2395\n",
            "           5       0.81      0.47      0.60      2360\n",
            "           6       0.94      0.79      0.85      2408\n",
            "           7       0.87      0.86      0.87      2349\n",
            "           8       0.84      0.66      0.74      2425\n",
            "           9       0.94      0.94      0.94      2460\n",
            "          10       0.98      0.84      0.90      2399\n",
            "          11       0.84      0.91      0.88      2359\n",
            "          12       0.90      0.82      0.86      2376\n",
            "          13       0.87      0.79      0.83      2365\n",
            "          14       0.83      0.68      0.75      2355\n",
            "          15       0.93      0.77      0.84      2410\n",
            "          16       0.76      0.79      0.78      2411\n",
            "          17       0.90      0.64      0.75      2475\n",
            "          18       0.92      0.83      0.87      2357\n",
            "          19       0.93      0.45      0.61      2421\n",
            "\n",
            "   micro avg       0.89      0.76      0.82     48000\n",
            "   macro avg       0.89      0.76      0.82     48000\n",
            "weighted avg       0.89      0.76      0.81     48000\n",
            " samples avg       0.76      0.76      0.76     48000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzSkC0mzPamd",
        "colab_type": "text"
      },
      "source": [
        "4. Preliminary results\n",
        "  \n",
        "  The confusion matrix above shows the performance of the model. Strangely, many items are being mistaken for the aircraft_carrier.It seems to be a little overfit. The accuracy rate of 76% is better than the 5% we'd expect from a machine blindly guessing the item. We can stt from the precision is much higher tha recall. We can conclude that the positives given are very likely to be true positives. But if we look at the recall, we notice an unbalanced outcome for each category of items (such as 19 vs 9). It might be because of the sample size used, and the hyperparameters. I believe based on the preliminary result that the aproach taken is plausible.\n",
        "\n",
        "5. Next steps\n",
        "\n",
        "  The model doe exceed my expectations of around 50 percent accuracy, but to make the Quickdraw Webapp, Fine tuning will be necessary. I will very likely have to fine tune the hyperparameters, potentially change the number convolutional layers and size of filters. \n",
        "\n",
        "  The model is a small, but essential part of the Webapp, I am aiming for around 80-85% accuracy for now. The next step will be find a way to create a canvas, and clean the raw data produced to feed to this model.\n"
      ]
    }
  ]
}